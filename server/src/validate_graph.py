#!/usr/bin/env python3
"""
Graph Quality Validation Script

Evaluates the quality of knowledge graphs generated by Graphiti
and determines if the chunking strategy is appropriate.
"""

import argparse
import asyncio
import json
from typing import Any

from mcp import ClientSession
from mcp.client.streamable_http import streamablehttp_client


async def get_graph_statistics(session: ClientSession) -> dict[str, Any]:
    stats = {}

    episodes_result = await session.call_tool(
        "get_episodes", arguments={"max_episodes": 1000, "group_ids": ["main"]}
    )
    episodes_data = episodes_result.structuredContent or json.loads(
        episodes_result.content[0].text
    )
    episodes = (
        episodes_data.get("result", {}).get("episodes", [])
        if isinstance(episodes_data, dict) and "result" in episodes_data
        else episodes_data.get("episodes", [])
    )
    stats["episode_count"] = len(episodes)

    if episodes:
        episode_lengths = [len(ep["content"]) for ep in episodes]
        stats["avg_episode_length"] = sum(episode_lengths) / len(episode_lengths)
        stats["min_episode_length"] = min(episode_lengths)
        stats["max_episode_length"] = max(episode_lengths)

    nodes_result = await session.call_tool(
        "search_nodes",
        arguments={
            "query": "GraphRAG OR Slack OR tonoyama OR entity",
            "max_nodes": 1000,
            "group_ids": ["main"],
        },
    )
    nodes_data = (
        nodes_result.structuredContent["result"]
        if nodes_result.structuredContent
        else json.loads(nodes_result.content[0].text)
    )
    nodes = nodes_data.get("nodes", [])
    stats["node_count"] = len(nodes)

    if nodes:
        node_labels = {}
        for node in nodes:
            for label in node.get("labels", []):
                node_labels[label] = node_labels.get(label, 0) + 1
        stats["node_labels"] = node_labels

        summary_lengths = [
            len(node.get("summary", "")) for node in nodes if node.get("summary")
        ]
        if summary_lengths:
            stats["avg_node_summary_length"] = sum(summary_lengths) / len(
                summary_lengths
            )

    facts_result = await session.call_tool(
        "search_memory_facts",
        arguments={
            "query": "GraphRAG OR Slack OR entity",
            "max_facts": 1000,
            "group_ids": ["main"],
        },
    )
    facts_data = (
        facts_result.structuredContent["result"]
        if facts_result.structuredContent
        else json.loads(facts_result.content[0].text)
    )
    facts = facts_data.get("facts", [])
    stats["fact_count"] = len(facts)

    if facts:
        fact_types = {}
        for fact in facts:
            fact_name = fact.get("name", "UNKNOWN")
            fact_types[fact_name] = fact_types.get(fact_name, 0) + 1
        stats["fact_types"] = fact_types

    return stats


def calculate_quality_score(stats: dict[str, Any]) -> dict[str, Any]:
    score = 0
    max_score = 0
    details = []

    episode_count = stats.get("episode_count", 0)
    node_count = stats.get("node_count", 0)
    fact_count = stats.get("fact_count", 0)

    if episode_count > 0:
        max_score += 10
        if node_count > 0:
            ratio = node_count / episode_count
            if 0.3 <= ratio <= 3.0:
                score += 10
                details.append(f"‚úÖ Node/Episode ratio: {ratio:.2f} (appropriate: 0.3-3.0)")
            else:
                score += 5
                details.append(f"‚ö†Ô∏è  Node/Episode ratio: {ratio:.2f} (recommended: 0.3-3.0)")
        else:
            details.append("‚ùå No nodes generated")

    max_score += 10
    if fact_count > 0:
        if node_count > 0:
            ratio = fact_count / node_count
            if ratio >= 1.0:
                score += 10
                details.append(f"‚úÖ Fact/Node ratio: {ratio:.2f} (good: >= 1.0)")
            elif ratio >= 0.5:
                score += 7
                details.append(f"‚ö†Ô∏è  Fact/Node ratio: {ratio:.2f} (improvable: 0.5-1.0)")
            else:
                score += 3
                details.append(f"‚ùå Fact/Node ratio: {ratio:.2f} (too low: < 0.5)")
        else:
            details.append("‚ùå No facts generated")
    else:
        details.append("‚ùå No facts (relationships) exist")

    max_score += 10
    avg_episode_length = stats.get("avg_episode_length", 0)
    if avg_episode_length > 0:
        if 200 <= avg_episode_length <= 2000:
            score += 10
            details.append(
                f"‚úÖ Average episode length: {avg_episode_length:.0f} chars (appropriate: 200-2000)"
            )
        elif 100 <= avg_episode_length < 200:
            score += 7
            details.append(
                f"‚ö†Ô∏è  Average episode length: {avg_episode_length:.0f} chars (short: 100-200)"
            )
        elif avg_episode_length < 100:
            score += 3
            details.append(
                f"‚ùå Average episode length: {avg_episode_length:.0f} chars (too short: < 100)"
            )
        else:
            score += 7
            details.append(
                f"‚ö†Ô∏è  Average episode length: {avg_episode_length:.0f} chars (long: > 2000)"
            )

    max_score += 10
    node_labels = stats.get("node_labels", {})
    if len(node_labels) > 0:
        entity_count = node_labels.get("Entity", 0)
        total_nodes = sum(node_labels.values())
        if entity_count > 0:
            entity_ratio = entity_count / total_nodes
            if entity_ratio > 0.8:
                score += 10
                details.append(f"‚úÖ Entity node ratio: {entity_ratio:.1%} (good)")
            else:
                score += 7
                details.append(f"‚ö†Ô∏è  Entity node ratio: {entity_ratio:.1%}")
        details.append(f"   Node label distribution: {node_labels}")

    max_score += 10
    fact_types = stats.get("fact_types", {})
    unique_fact_types = len(fact_types)
    if unique_fact_types >= 3:
        score += 10
        details.append(f"‚úÖ Fact type count: {unique_fact_types} (diverse)")
    elif unique_fact_types >= 1:
        score += 5
        details.append(f"‚ö†Ô∏è  Fact type count: {unique_fact_types} (low diversity)")
    else:
        details.append("‚ùå No fact types exist")

    if fact_types:
        top_fact_types = sorted(fact_types.items(), key=lambda x: x[1], reverse=True)[
            :5
        ]
        details.append(
            f"   Main fact types: {', '.join([f'{k}({v})' for k, v in top_fact_types])}"
        )

    return {
        "score": score,
        "max_score": max_score,
        "percentage": (score / max_score * 100) if max_score > 0 else 0,
        "details": details,
    }


async def validate_graph(verbose: bool = False) -> None:
    async with streamablehttp_client("http://localhost:8001/mcp/") as (read, write, _):
        async with ClientSession(read, write) as session:
            await session.initialize()

            print("=" * 70)
            print("Graphiti Knowledge Graph Quality Validation")
            print("=" * 70)

            stats = await get_graph_statistics(session)

            print("\nüìä Basic Statistics:")
            print(f"  Episode count: {stats.get('episode_count', 0)}")
            print(f"  Node count: {stats.get('node_count', 0)}")
            print(f"  Fact count: {stats.get('fact_count', 0)}")

            if verbose:
                print("\nüìè Detailed Statistics:")
                if "avg_episode_length" in stats:
                    print(f"  Average episode length: {stats['avg_episode_length']:.0f} chars")
                    print(
                        f"  Min episode length: {stats.get('min_episode_length', 0)} chars"
                    )
                    print(
                        f"  Max episode length: {stats.get('max_episode_length', 0)} chars"
                    )
                if "avg_node_summary_length" in stats:
                    print(
                        f"  Average node summary length: {stats['avg_node_summary_length']:.0f} chars"
                    )

            quality = calculate_quality_score(stats)

            print("\nüéØ Quality Score:")
            print(
                f"  {quality['score']}/{quality['max_score']} ({quality['percentage']:.1f}%)"
            )

            print("\nüìã Evaluation Details:")
            for detail in quality["details"]:
                print(f"  {detail}")

            print("\nüí° Overall Evaluation:")
            percentage = quality["percentage"]
            if percentage >= 80:
                print("  ‚úÖ Excellent - Graph is high quality")
            elif percentage >= 60:
                print("  ‚ö†Ô∏è  Good - Some room for improvement")
            elif percentage >= 40:
                print("  ‚ö†Ô∏è  Needs Improvement - Recommend reviewing chunking strategy")
            else:
                print("  ‚ùå Poor - Issues with data ingestion or chunking")

            print("\nüìù Improvement Suggestions:")
            if stats.get("episode_count", 0) == 0:
                print(
                    "  - No data has been ingested. Run load_slack_data.py"
                )
            elif stats.get("node_count", 0) == 0:
                print("  - No nodes generated. Check LLM configuration")
            elif stats.get("fact_count", 0) == 0:
                print(
                    "  - No relationships extracted. Make episode content more conversational"
                )
            else:
                avg_length = stats.get("avg_episode_length", 0)
                if avg_length < 100:
                    print(
                        "  - Episodes too short. Increase time window (e.g., 4H, 1D)"
                    )
                elif avg_length > 2000:
                    print(
                        "  - Episodes too long. Decrease time window (e.g., 30min, 1H)"
                    )

                node_ratio = stats.get("node_count", 0) / stats.get("episode_count", 1)
                if node_ratio < 0.3:
                    print(
                        "  - Too few nodes. Create episodes with more specific content"
                    )
                elif node_ratio > 3.0:
                    print(
                        "  - Too many nodes. Make episodes more cohesive"
                    )

                fact_ratio = stats.get("fact_count", 0) / max(
                    stats.get("node_count", 1), 1
                )
                if fact_ratio < 0.5:
                    print(
                        "  - Too few relationships. Recommend thread-based chunking"
                    )

            print("\n" + "=" * 70)


async def main():
    parser = argparse.ArgumentParser(
        description="Validate Graphiti knowledge graph quality."
    )
    parser.add_argument(
        "-v", "--verbose", action="store_true", help="Display detailed statistics."
    )
    args = parser.parse_args()

    await validate_graph(verbose=args.verbose)


if __name__ == "__main__":
    asyncio.run(main())
