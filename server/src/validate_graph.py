#!/usr/bin/env python3
"""
ã‚°ãƒ©ãƒ•å“è³ªæ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Graphitiã§ç”Ÿæˆã•ã‚ŒãŸãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã®å“è³ªã‚’è©•ä¾¡ã—ã€
ãƒãƒ£ãƒ³ã‚¯åŒ–æˆ¦ç•¥ãŒé©åˆ‡ã‹ã©ã†ã‹ã‚’åˆ¤å®šã—ã¾ã™ã€‚
"""

import argparse
import asyncio
import json
from typing import Any

from mcp import ClientSession
from mcp.client.streamable_http import streamablehttp_client


async def get_graph_statistics(session: ClientSession) -> dict[str, Any]:
    stats = {}

    episodes_result = await session.call_tool(
        "get_episodes", arguments={"max_episodes": 1000, "group_ids": ["main"]}
    )
    episodes_data = episodes_result.structuredContent or json.loads(
        episodes_result.content[0].text
    )
    episodes = (
        episodes_data.get("result", {}).get("episodes", [])
        if isinstance(episodes_data, dict) and "result" in episodes_data
        else episodes_data.get("episodes", [])
    )
    stats["episode_count"] = len(episodes)

    if episodes:
        episode_lengths = [len(ep["content"]) for ep in episodes]
        stats["avg_episode_length"] = sum(episode_lengths) / len(episode_lengths)
        stats["min_episode_length"] = min(episode_lengths)
        stats["max_episode_length"] = max(episode_lengths)

    nodes_result = await session.call_tool(
        "search_nodes",
        arguments={
            "query": "GraphRAG OR Slack OR tonoyama OR entity",
            "max_nodes": 1000,
            "group_ids": ["main"],
        },
    )
    nodes_data = (
        nodes_result.structuredContent["result"]
        if nodes_result.structuredContent
        else json.loads(nodes_result.content[0].text)
    )
    nodes = nodes_data.get("nodes", [])
    stats["node_count"] = len(nodes)

    if nodes:
        node_labels = {}
        for node in nodes:
            for label in node.get("labels", []):
                node_labels[label] = node_labels.get(label, 0) + 1
        stats["node_labels"] = node_labels

        summary_lengths = [
            len(node.get("summary", "")) for node in nodes if node.get("summary")
        ]
        if summary_lengths:
            stats["avg_node_summary_length"] = sum(summary_lengths) / len(
                summary_lengths
            )

    facts_result = await session.call_tool(
        "search_memory_facts",
        arguments={
            "query": "GraphRAG OR Slack OR entity",
            "max_facts": 1000,
            "group_ids": ["main"],
        },
    )
    facts_data = (
        facts_result.structuredContent["result"]
        if facts_result.structuredContent
        else json.loads(facts_result.content[0].text)
    )
    facts = facts_data.get("facts", [])
    stats["fact_count"] = len(facts)

    if facts:
        fact_types = {}
        for fact in facts:
            fact_name = fact.get("name", "UNKNOWN")
            fact_types[fact_name] = fact_types.get(fact_name, 0) + 1
        stats["fact_types"] = fact_types

    return stats


def calculate_quality_score(stats: dict[str, Any]) -> dict[str, Any]:
    score = 0
    max_score = 0
    details = []

    episode_count = stats.get("episode_count", 0)
    node_count = stats.get("node_count", 0)
    fact_count = stats.get("fact_count", 0)

    if episode_count > 0:
        max_score += 10
        if node_count > 0:
            ratio = node_count / episode_count
            if 0.3 <= ratio <= 3.0:
                score += 10
                details.append(f"âœ… ãƒãƒ¼ãƒ‰/ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ¯”: {ratio:.2f} (é©åˆ‡: 0.3-3.0)")
            else:
                score += 5
                details.append(f"âš ï¸  ãƒãƒ¼ãƒ‰/ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ¯”: {ratio:.2f} (æ¨å¥¨: 0.3-3.0)")
        else:
            details.append("âŒ ãƒãƒ¼ãƒ‰ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“")

    max_score += 10
    if fact_count > 0:
        if node_count > 0:
            ratio = fact_count / node_count
            if ratio >= 1.0:
                score += 10
                details.append(f"âœ… ãƒ•ã‚¡ã‚¯ãƒˆ/ãƒãƒ¼ãƒ‰æ¯”: {ratio:.2f} (è‰¯å¥½: >= 1.0)")
            elif ratio >= 0.5:
                score += 7
                details.append(f"âš ï¸  ãƒ•ã‚¡ã‚¯ãƒˆ/ãƒãƒ¼ãƒ‰æ¯”: {ratio:.2f} (æ”¹å–„å¯èƒ½: 0.5-1.0)")
            else:
                score += 3
                details.append(f"âŒ ãƒ•ã‚¡ã‚¯ãƒˆ/ãƒãƒ¼ãƒ‰æ¯”: {ratio:.2f} (ä½ã™ãã‚‹: < 0.5)")
        else:
            details.append("âŒ ãƒ•ã‚¡ã‚¯ãƒˆãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“")
    else:
        details.append("âŒ ãƒ•ã‚¡ã‚¯ãƒˆï¼ˆé–¢ä¿‚æ€§ï¼‰ãŒå­˜åœ¨ã—ã¾ã›ã‚“")

    max_score += 10
    avg_episode_length = stats.get("avg_episode_length", 0)
    if avg_episode_length > 0:
        if 200 <= avg_episode_length <= 2000:
            score += 10
            details.append(
                f"âœ… å¹³å‡ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·: {avg_episode_length:.0f}æ–‡å­— (é©åˆ‡: 200-2000)"
            )
        elif 100 <= avg_episode_length < 200:
            score += 7
            details.append(
                f"âš ï¸  å¹³å‡ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·: {avg_episode_length:.0f}æ–‡å­— (çŸ­ã„: 100-200)"
            )
        elif avg_episode_length < 100:
            score += 3
            details.append(
                f"âŒ å¹³å‡ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·: {avg_episode_length:.0f}æ–‡å­— (çŸ­ã™ãã‚‹: < 100)"
            )
        else:
            score += 7
            details.append(
                f"âš ï¸  å¹³å‡ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·: {avg_episode_length:.0f}æ–‡å­— (é•·ã„: > 2000)"
            )

    max_score += 10
    node_labels = stats.get("node_labels", {})
    if len(node_labels) > 0:
        entity_count = node_labels.get("Entity", 0)
        total_nodes = sum(node_labels.values())
        if entity_count > 0:
            entity_ratio = entity_count / total_nodes
            if entity_ratio > 0.8:
                score += 10
                details.append(f"âœ… ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒãƒ¼ãƒ‰æ¯”ç‡: {entity_ratio:.1%} (è‰¯å¥½)")
            else:
                score += 7
                details.append(f"âš ï¸  ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒãƒ¼ãƒ‰æ¯”ç‡: {entity_ratio:.1%}")
        details.append(f"   ãƒãƒ¼ãƒ‰ãƒ©ãƒ™ãƒ«åˆ†å¸ƒ: {node_labels}")

    max_score += 10
    fact_types = stats.get("fact_types", {})
    unique_fact_types = len(fact_types)
    if unique_fact_types >= 3:
        score += 10
        details.append(f"âœ… ãƒ•ã‚¡ã‚¯ãƒˆã‚¿ã‚¤ãƒ—æ•°: {unique_fact_types} (å¤šæ§˜æ€§ã‚ã‚Š)")
    elif unique_fact_types >= 1:
        score += 5
        details.append(f"âš ï¸  ãƒ•ã‚¡ã‚¯ãƒˆã‚¿ã‚¤ãƒ—æ•°: {unique_fact_types} (å¤šæ§˜æ€§ãŒä½ã„)")
    else:
        details.append("âŒ ãƒ•ã‚¡ã‚¯ãƒˆã‚¿ã‚¤ãƒ—ãŒå­˜åœ¨ã—ã¾ã›ã‚“")

    if fact_types:
        top_fact_types = sorted(fact_types.items(), key=lambda x: x[1], reverse=True)[
            :5
        ]
        details.append(
            f"   ä¸»ãªãƒ•ã‚¡ã‚¯ãƒˆã‚¿ã‚¤ãƒ—: {', '.join([f'{k}({v})' for k, v in top_fact_types])}"
        )

    return {
        "score": score,
        "max_score": max_score,
        "percentage": (score / max_score * 100) if max_score > 0 else 0,
        "details": details,
    }


async def validate_graph(verbose: bool = False) -> None:
    async with streamablehttp_client("http://localhost:8001/mcp/") as (read, write, _):
        async with ClientSession(read, write) as session:
            await session.initialize()

            print("=" * 70)
            print("Graphiti ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•å“è³ªæ¤œè¨¼")
            print("=" * 70)

            stats = await get_graph_statistics(session)

            print("\nğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
            print(f"  ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {stats.get('episode_count', 0)}")
            print(f"  ãƒãƒ¼ãƒ‰æ•°: {stats.get('node_count', 0)}")
            print(f"  ãƒ•ã‚¡ã‚¯ãƒˆæ•°: {stats.get('fact_count', 0)}")

            if verbose:
                print("\nğŸ“ è©³ç´°çµ±è¨ˆ:")
                if "avg_episode_length" in stats:
                    print(f"  å¹³å‡ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·: {stats['avg_episode_length']:.0f}æ–‡å­—")
                    print(
                        f"  æœ€å°ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·: {stats.get('min_episode_length', 0)}æ–‡å­—"
                    )
                    print(
                        f"  æœ€å¤§ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·: {stats.get('max_episode_length', 0)}æ–‡å­—"
                    )
                if "avg_node_summary_length" in stats:
                    print(
                        f"  å¹³å‡ãƒãƒ¼ãƒ‰è¦ç´„é•·: {stats['avg_node_summary_length']:.0f}æ–‡å­—"
                    )

            quality = calculate_quality_score(stats)

            print("\nğŸ¯ å“è³ªã‚¹ã‚³ã‚¢:")
            print(
                f"  {quality['score']}/{quality['max_score']} ({quality['percentage']:.1f}%)"
            )

            print("\nğŸ“‹ è©•ä¾¡è©³ç´°:")
            for detail in quality["details"]:
                print(f"  {detail}")

            print("\nğŸ’¡ ç·åˆè©•ä¾¡:")
            percentage = quality["percentage"]
            if percentage >= 80:
                print("  âœ… å„ªç§€ - ã‚°ãƒ©ãƒ•ã¯é«˜å“è³ªã§ã™")
            elif percentage >= 60:
                print("  âš ï¸  è‰¯å¥½ - ã„ãã¤ã‹æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™")
            elif percentage >= 40:
                print("  âš ï¸  è¦æ”¹å–„ - ãƒãƒ£ãƒ³ã‚¯åŒ–æˆ¦ç•¥ã®è¦‹ç›´ã—ã‚’æ¨å¥¨ã—ã¾ã™")
            else:
                print("  âŒ ä¸è‰¯ - ãƒ‡ãƒ¼ã‚¿æŠ•å…¥ã¾ãŸã¯ãƒãƒ£ãƒ³ã‚¯åŒ–ã«å•é¡ŒãŒã‚ã‚Šã¾ã™")

            print("\nğŸ“ æ”¹å–„ææ¡ˆ:")
            if stats.get("episode_count", 0) == 0:
                print(
                    "  - ãƒ‡ãƒ¼ã‚¿ãŒæŠ•å…¥ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚load_slack_data.pyã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„"
                )
            elif stats.get("node_count", 0) == 0:
                print("  - ãƒãƒ¼ãƒ‰ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚LLMè¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
            elif stats.get("fact_count", 0) == 0:
                print(
                    "  - é–¢ä¿‚æ€§ãŒæŠ½å‡ºã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å†…å®¹ã‚’ã‚ˆã‚Šä¼šè©±çš„ã«ã—ã¦ãã ã•ã„"
                )
            else:
                avg_length = stats.get("avg_episode_length", 0)
                if avg_length < 100:
                    print(
                        "  - ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãŒçŸ­ã™ãã¾ã™ã€‚æ™‚é–“çª“ã‚’é•·ãï¼ˆä¾‹: 4H, 1Dï¼‰ã—ã¦ãã ã•ã„"
                    )
                elif avg_length > 2000:
                    print(
                        "  - ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãŒé•·ã™ãã¾ã™ã€‚æ™‚é–“çª“ã‚’çŸ­ãï¼ˆä¾‹: 30min, 1Hï¼‰ã—ã¦ãã ã•ã„"
                    )

                node_ratio = stats.get("node_count", 0) / stats.get("episode_count", 1)
                if node_ratio < 0.3:
                    print(
                        "  - ãƒãƒ¼ãƒ‰æ•°ãŒå°‘ãªã™ãã¾ã™ã€‚ã‚ˆã‚Šå…·ä½“çš„ãªå†…å®¹ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦ãã ã•ã„"
                    )
                elif node_ratio > 3.0:
                    print(
                        "  - ãƒãƒ¼ãƒ‰æ•°ãŒå¤šã™ãã¾ã™ã€‚ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ã‚ˆã‚Šå‡é›†ã•ã›ã¦ãã ã•ã„"
                    )

                fact_ratio = stats.get("fact_count", 0) / max(
                    stats.get("node_count", 1), 1
                )
                if fact_ratio < 0.5:
                    print(
                        "  - é–¢ä¿‚æ€§ãŒå°‘ãªã™ãã¾ã™ã€‚ã‚¹ãƒ¬ãƒƒãƒ‰å˜ä½ã§ã®ãƒãƒ£ãƒ³ã‚¯åŒ–ã‚’æ¨å¥¨ã—ã¾ã™"
                    )

            print("\n" + "=" * 70)


async def main():
    parser = argparse.ArgumentParser(
        description="GraphitiãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã®å“è³ªã‚’æ¤œè¨¼ã—ã¾ã™ã€‚"
    )
    parser.add_argument(
        "-v", "--verbose", action="store_true", help="è©³ç´°ãªçµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚"
    )
    args = parser.parse_args()

    await validate_graph(verbose=args.verbose)


if __name__ == "__main__":
    asyncio.run(main())
