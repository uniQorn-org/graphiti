# Graphiti MCP Server Environment Configuration

# Neo4j Database Configuration
# These settings are used to connect to your Neo4j database
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password123

# Database Provider Configuration
DATABASE__PROVIDER=neo4j
DATABASE__PROVIDERS__NEO4J__URI=bolt://localhost:7687
DATABASE__PROVIDERS__NEO4J__USERNAME=neo4j
DATABASE__PROVIDERS__NEO4J__PASSWORD=password123

# OpenAI API Configuration
# Required for LLM operations
OPENAI_API_KEY=your_openai_api_key_here
MODEL_NAME=gpt-4o-mini

# LLM Provider Configuration
LLM__PROVIDER=openai
LLM__MODEL=gpt-4o-mini
LLM__PROVIDERS__OPENAI__API_KEY=your_openai_api_key_here

# Embedder Provider Configuration
EMBEDDER__PROVIDER=openai
EMBEDDER__MODEL=text-embedding-3-small
EMBEDDER__PROVIDERS__OPENAI__API_KEY=your_openai_api_key_here

# Server Configuration
SERVER__PORT=8001

# Search Bot Backend Configuration
BACKEND_PORT=20001
CORS_ORIGINS=http://localhost:20002,http://localhost:3000,http://localhost:5173

# Optional: Only needed for non-standard OpenAI endpoints
# OPENAI_BASE_URL=https://api.openai.com/v1

# Optional: Group ID for namespacing graph data
# GROUP_ID=my_project

# Concurrency Control
# Controls how many episodes can be processed simultaneously
# Default: 10 (suitable for OpenAI Tier 3, mid-tier Anthropic)
# Adjust based on your LLM provider's rate limits:
#   - OpenAI Tier 1 (free): 1-2
#   - OpenAI Tier 2: 5-8
#   - OpenAI Tier 3: 10-15
#   - OpenAI Tier 4: 20-50
#   - Anthropic default: 5-8
#   - Anthropic high tier: 15-30
#   - Ollama (local): 1-5
# See README.md "Concurrency and LLM Provider 429 Rate Limit Errors" for details
SEMAPHORE_LIMIT=10

# Optional: Path configuration for Docker
# PATH=/root/.local/bin:${PATH}

# Optional: Memory settings for Neo4j (used in Docker Compose)
# NEO4J_server_memory_heap_initial__size=512m
# NEO4J_server_memory_heap_max__size=1G
# NEO4J_server_memory_pagecache_size=512m

# Azure OpenAI configuration
# Optional: Only needed for Azure OpenAI endpoints
# AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint_here
# AZURE_OPENAI_API_VERSION=2025-01-01-preview
# AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o-deployment
# AZURE_OPENAI_EMBEDDING_API_VERSION=2023-05-15
# AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME=text-embedding-3-small-deployment
# AZURE_OPENAI_USE_MANAGED_IDENTITY=false

# Corporate Proxy Configuration
# Optional: Configure proxy settings for OpenAI API access
# Useful when running behind corporate firewalls or for testing purposes
# PROXY_USE=FALSE                           # Set to TRUE to enable proxy (FALSE for local development)
# OPENAI_PROXY=http://proxy.company.com:8080
# OPENAI_PROXY_USERNAME=                    # Optional: Username for proxy authentication
# OPENAI_PROXY_PASSWORD=                    # Optional: Password for proxy authentication
# NO_PROXY=localhost,127.0.0.1,neo4j,minio,graphiti-mcp  # Comma-separated list of hosts to bypass proxy